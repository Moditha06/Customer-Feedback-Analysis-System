{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTJvmhQUIYvb8L6hbSZI0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moditha06/Customer-Feedback-Analysis-System/blob/main/MLE2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Customer Feedback Analysis System**"
      ],
      "metadata": {
        "id": "roJ5KXmWVHI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "This project involves creating a comprehensive machine learning pipeline to analyze customer feedback data. Here's an approach to building and deploying the system:*italicized text*"
      ],
      "metadata": {
        "id": "eju85hjES438"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Ingestion and Preprocessing\n",
        "Tasks:**\n",
        "\n",
        "Data Collection: Create a system to ingest data from multiple sources (CSV files, APIs). Utilize the Kaggle dataset: Amazon Product Reviews.\n",
        "\n",
        "Cleaning: Remove duplicates, handle missing values, normalize text (e.g., lowercase, remove stopwords, punctuation).\n",
        "\n",
        "Handling Imbalanced Data: Apply oversampling (e.g., SMOTE) or class weighting in model training.\n",
        "\n",
        "\n",
        "**Implementation Plan:**\n",
        "\n",
        "Use Python with Pandas for data ingestion and cleaning.\n",
        "\n",
        "1.   Use Python with Pandas for data ingestion and cleaning.\n",
        "2.   For text normalization, use libraries like NLTK or SpaCy.\n",
        "3.   Use scikit-learn for handling imbalanced datasets."
      ],
      "metadata": {
        "id": "Jtv6cchQTKyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jqc_3VZJILYe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import re\n",
        "import string\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYdhENnNPgrg",
        "outputId": "2cad7dd4-f9bd-41c3-819c-288adc92f714"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Ingestion and Preprocessing\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    text = re.sub(\"\\\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_and_preprocess_data(/content/drive/MyDrive/Reviews[1].csv):\n",
        "    data = pd.read_csv(/content/drive/MyDrive/Reviews[1].csv)\n",
        "    data.drop_duplicates(inplace=True)\n",
        "    data = data.dropna(subset=['reviewText', 'overall'])\n",
        "    data['cleaned_text'] = data['reviewText'].apply(clean_text)\n",
        "    data['label'] = data['overall'].apply(lambda x: 'positive' if x > 3 else 'negative' if x < 3 else 'neutral')\n",
        "    return data[['cleaned_text', 'label']]"
      ],
      "metadata": {
        "id": "N8f6qzPMVkPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model Development\n",
        "Tasks:**\n",
        "\n",
        "**Model Selection:**\n",
        "Traditional ML (e.g., Logistic Regression, Random Forest) for simplicity and interpretability.\n",
        "\n",
        "Deep Learning (e.g., BERT) for better performance on text classification.\n",
        "\n",
        "**Training and Evaluation:**\n",
        "Split data into training, validation, and test sets.\n",
        "\n",
        "Use metrics like precision, recall, and F1 score to evaluate performance.\n",
        "\n",
        "**Implementation Plan:**\n",
        "\n",
        "1. Explore TF-IDF and embeddings (e.g., Word2Vec, BERT).\n",
        "2. Train models using libraries like scikit-learn, TensorFlow, or PyTorch.\n",
        "3. Use GridSearchCV for hyperparameter tuning."
      ],
      "metadata": {
        "id": "AiKQsno3V1Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Engineering and Splitting\n",
        "def feature_engineering(data):\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "    y = data['label']\n",
        "    return X, y, vectorizer"
      ],
      "metadata": {
        "id": "uYS8NoVBQEud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Feature Engineering\n",
        "Tasks:**\n",
        "\n",
        "Perform EDA to understand dataset distributions, identify class imbalances, and determine feature importance.\n",
        "\n",
        "**Extract features using:**\n",
        "1. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "2. Pre-trained embeddings like BERT for semantic understanding.\n",
        "\n",
        "**Implementation Plan:**\n",
        "\n",
        "1. Visualize data with Matplotlib or Seaborn.\n",
        "2. Use scikit-learn or transformers library for feature extraction."
      ],
      "metadata": {
        "id": "3U2ik12mWoI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Model Training\n",
        "def train_model(X, y):\n",
        "    smote = SMOTE()\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "q6XkMM3AQKRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model Deployment\n",
        "Tasks:**\n",
        "\n",
        "1. Build an API for real-time predictions using FastAPI or Flask.\n",
        "\n",
        "2. Deploy on a cloud platform like AWS, Google Cloud, or Heroku.\n",
        "\n",
        "3. Ensure low latency and high concurrency using containerization with Docker.\n",
        "\n",
        "**Implementation Plan:**\n",
        "\n",
        "1. Create endpoints for classification predictions.\n",
        "2. Use Gunicorn with Docker to handle concurrent requests.\n"
      ],
      "metadata": {
        "id": "5zGSEFAAXiZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: API Deployment\n",
        "def create_api(model, vectorizer):\n",
        "    app = Flask(__name__)\n",
        "\n",
        "    @app.route('/predict', methods=['POST'])\n",
        "    def predict():\n",
        "        if request.method == 'POST':\n",
        "            input_data = request.json.get('feedback', '')\n",
        "            if not input_data:\n",
        "                return jsonify({'error': 'No feedback provided'}), 400\n",
        "            cleaned_input = clean_text(input_data)\n",
        "            features = vectorizer.transform([cleaned_input])\n",
        "            prediction = model.predict(features)[0]\n",
        "            return jsonify({'prediction': prediction})\n",
        "\n",
        "    return app"
      ],
      "metadata": {
        "id": "y-VT7VM4QO4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Monitoring and Feedback Loop\n",
        "Tasks:**\n",
        "\n",
        "1. Track model performance metrics (accuracy, precision, recall) in production.\n",
        "\n",
        "2. Monitor data drift and implement alerts for significant deviations.\n",
        "\n",
        "3. Periodically retrain the model with new data.\n",
        "\n",
        "**Implementation Plan:**\n",
        "\n",
        "1. Use Prometheus and Grafana for monitoring.\n",
        "\n",
        "2. Automate retraining using a CI/CD pipeline with tools like GitHub Actions or Jenkin"
      ],
      "metadata": {
        "id": "IxZr8yzsbMpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Monitoring and Feedback Loop\n",
        "def retrain_model_with_new_data(new_data_filepath, model, vectorizer):\n",
        "    new_data = load_and_preprocess_data(new_data_filepath)\n",
        "    X_new, y_new, _ = feature_engineering(new_data)\n",
        "    X_resampled, y_resampled = SMOTE().fit_resample(X_new, y_new)\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "    return model"
      ],
      "metadata": {
        "id": "U_0E-uvAQTCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution flow\n",
        "if __name__ == '__main__':\n",
        "    # Load and preprocess data\n",
        "    data_filepath = 'amazon_reviews.csv'  # Replace with actual dataset path\n",
        "    data = load_and_preprocess_data(data_filepath)\n",
        "\n",
        "    # Feature engineering and model training\n",
        "    X, y, vectorizer = feature_engineering(data)\n",
        "    model, vectorizer = train_model(X, y)\n",
        "\n",
        "    # Save the model and vectorizer\n",
        "    joblib.dump(model, 'feedback_model.pkl')\n",
        "    joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "\n",
        "    # Create and run the API\n",
        "    app = create_api(model, vectorizer)\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "ySv6Sgt8Qaek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Plan\n",
        "Metrics:**\n",
        "\n",
        "1. Precision, Recall, F1 Score for model evaluation.\n",
        "\n",
        "2. Latency and throughput for API performance.\n",
        "\n",
        "**Scalability:**\n",
        "\n",
        "Deploy a scalable architecture using container orchestration tools like Kubernetes."
      ],
      "metadata": {
        "id": "FnL_C8-Scj6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps**\n",
        "\n",
        "1. Set up project repository: Structure code and documents.\n",
        "\n",
        "2. Collect and preprocess data: Start with the Kaggle dataset and preprocess it.\n",
        "\n",
        "3. Train initial models: Use simple models (e.g., Logistic Regression) and progressively test advanced models (e.g., BERT).\n",
        "\n",
        "4. Build API: Create and test the prediction endpoint.\n",
        "\n",
        "5. Deploy and monitor: Host API and implement monitoring systems."
      ],
      "metadata": {
        "id": "S59nNtGPc1AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iâ€™ve built the Python code for the customer feedback analysis system following your requirements. The code includes data ingestion, preprocessing, model training, and API deployment.\n",
        "\n",
        " Key features:\n",
        "\n",
        "1. Data Preprocessing: Cleans and normalizes text reviews.\n",
        "2. Feature Engineering: Uses TF-IDF for feature extraction.\n",
        "3. Model Training: Employs a Random Forest model with SMOTE for handling class imbalance.\n",
        "4. API Deployment: A Flask-based API for real-time predictions."
      ],
      "metadata": {
        "id": "I6vwyam4fXrI"
      }
    }
  ]
}